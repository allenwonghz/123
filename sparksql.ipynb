{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(age=11, name='Alice')\n",
      "Alice 11\n",
      "Alice 11\n",
      "<built-in method count of Row object at 0x10cd59470>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(name=\"Alice\", age=11)\n",
    "print row\n",
    "print row['name'], row['age']\n",
    "print row.name, row.age\n",
    "\n",
    "row = Row(name=\"Alice\", age=11, count=1)\n",
    "print row.count\n",
    "print row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data file at https://www.cse.ust.hk/msbd/data/building.csv\n",
    "df = spark.read.csv('../data/building.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the content of the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuildingID: integer (nullable = true)\n",
      " |-- BuildingMgr: string (nullable = true)\n",
      " |-- BuildingAge: integer (nullable = true)\n",
      " |-- HVACproduct: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe schema in a tree format\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(BuildingID=1, BuildingMgr=u'M1', BuildingAge=25, HVACproduct=u'AC1000', Country=u'USA'), Row(BuildingID=2, BuildingMgr=u'M2', BuildingAge=27, HVACproduct=u'FN39TG', Country=u'France'), Row(BuildingID=3, BuildingMgr=u'M3', BuildingAge=28, HVACproduct=u'JDNS77', Country=u'Brazil')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an RDD from the dataframe\n",
    "dfrdd = df.rdd\n",
    "dfrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|BuildingID|Country|\n",
      "+----------+-------+\n",
      "|         1|    USA|\n",
      "|         2| France|\n",
      "|         3| Brazil|\n",
      "+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve specific columns from the dataframe\n",
    "df.select('BuildingID', 'Country').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|Country| OK|\n",
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "|         1|         M1|         25|     AC1000|    USA| OK|\n",
      "+----------+-----------+-----------+-----------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.where(\"Country='USA'\").select('*', lit('OK')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACProduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    4|\n",
      "|     AC1000|    4|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GroupBy clause with dataframe \n",
    "df.groupBy('HVACProduct').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting SQL with DataFrame API\n",
    "\n",
    "The data files have been put to a public blob container, which can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "# Data files at https://www.cse.ust.hk/msbd5003/data\n",
    "\n",
    "dfCustomer = spark.read.csv('../data/Customer.csv', header=True, inferSchema=True)\n",
    "dfProduct = spark.read.csv('../data/Product.csv', header=True, inferSchema=True)\n",
    "dfDetail = spark.read.csv('../data/SalesOrderDetail.csv', header=True, inferSchema=True)\n",
    "dfHeader = spark.read.csv('../data/SalesOrderHeader.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------+\n",
      "|ProductID|Name                     |ListPrice|\n",
      "+---------+-------------------------+---------+\n",
      "|680      |HL Road Frame - Black, 58|1431.5   |\n",
      "|708      |Sport-100 Helmet, Black  |34.99    |\n",
      "|722      |LL Road Frame - Black, 58|337.22   |\n",
      "+---------+-------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black'\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+-----------+\n",
      "|ProductID|Name                     |DoublePrice|\n",
      "+---------+-------------------------+-----------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0     |\n",
      "|708      |Sport-100 Helmet, Black  |69.98      |\n",
      "|722      |LL Road Frame - Black, 58|674.44     |\n",
      "+---------+-------------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dfProduct['ProductID'] is same as dfProduct.ProductID\n",
    "# alias corresponds as in SQL\n",
    "dfProduct.where(dfProduct.Color=='Black') \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], (dfProduct.ListPrice * 2).alias('DoublePrice')) \\\n",
    "         .show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+---------------+\n",
      "|ProductID|Name                     |(ListPrice * 2)|\n",
      "+---------+-------------------------+---------------+\n",
      "|680      |HL Road Frame - Black, 58|2863.0         |\n",
      "|706      |HL Road Frame - Red, 58  |2863.0         |\n",
      "|717      |HL Road Frame - Red, 62  |2863.0         |\n",
      "+---------+-------------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfProduct.where(dfProduct.ListPrice * 2 > 100) \\\n",
    "         .select(dfProduct.ProductID, dfProduct['Name'], dfProduct.ListPrice * 2) \\\n",
    "         .show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------+---------+\n",
      "|ProductID|Name                 |ListPrice|\n",
      "+---------+---------------------+---------+\n",
      "|860      |Half-Finger Gloves, L|24.49    |\n",
      "|858      |Half-Finger Gloves, S|24.49    |\n",
      "|859      |Half-Finger Gloves, M|24.49    |\n",
      "+---------+---------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT ProductID, Name, ListPrice \n",
    "# FROM Product \n",
    "# WHERE Color = 'black' \n",
    "# ORDER BY ProductID\n",
    "\n",
    "dfProduct.filter(\"Color = 'Black'\")\\\n",
    "         .select('ProductID', 'Name', 'ListPrice')\\\n",
    "         .orderBy('ListPrice')\\\n",
    "         .show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71780|            110620|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110621|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders and details on black product,\n",
    "# return the product SalesOrderID, SalesOrderDetailID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesOrderDetail.ProductID = Product.ProductID AND Color = 'Black'\n",
    "\n",
    "# SELECT SalesOrderID, SalesOrderDetailID, Name, UnitPrice, OrderQty \n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "# Spark SQL supports natural joins\n",
    "\n",
    "dfDetail.join(dfProduct, 'ProductID') \\\n",
    "        .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty') \\\n",
    "        .filter(\"Color='Black'\")\\\n",
    "        .show(3)\n",
    "\n",
    "# If we move the filter to after select, it still works.  Why?\n",
    "#  lazy evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+--------------------+---------+--------+\n",
      "|SalesOrderID|SalesOrderDetailID|                Name|UnitPrice|OrderQty|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "|       71780|            110620|Mountain-500 Blac...|  323.994|       1|\n",
      "|       71780|            110621|LL Mountain Frame...|  149.874|       1|\n",
      "|       71780|            110622|HL Mountain Frame...|   809.76|       1|\n",
      "+------------+------------------+--------------------+---------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [SalesOrderID#175, SalesOrderDetailID#176, Name#129, UnitPrice#179, OrderQty#177]\n",
      "+- *BroadcastHashJoin [ProductID#178], [ProductID#128], Inner, BuildRight\n",
      "   :- *Project [SalesOrderID#175, SalesOrderDetailID#176, OrderQty#177, ProductID#178, UnitPrice#179]\n",
      "   :  +- *Filter isnotnull(ProductID#178)\n",
      "   :     +- *FileScan csv [SalesOrderID#175,SalesOrderDetailID#176,OrderQty#177,ProductID#178,UnitPrice#179] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/allenwong/Desktop/BDT/5003BigDataComputing/data/SalesOrderDetail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<SalesOrderID:int,SalesOrderDetailID:int,OrderQty:int,ProductID:int,UnitPrice:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      "      +- *Project [ProductID#128, Name#129]\n",
      "         +- *Filter ((isnotnull(Color#131) && (Color#131 = Black)) && isnotnull(ProductID#128))\n",
      "            +- *FileScan csv [ProductID#128,Name#129,Color#131] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/allenwong/Desktop/BDT/5003BigDataComputing/data/Product.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Color), EqualTo(Color,Black), IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,Name:string,Color:string>\n"
     ]
    }
   ],
   "source": [
    "# This also works:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d2 = d1.filter(\"Color = 'Black'\")\n",
    "d2.show(3)\n",
    "d2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will report an error:\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID') \\\n",
    "             .select('SalesOrderID', 'SalesOrderDetailID', 'Name', 'UnitPrice', 'OrderQty')\n",
    "d1.write.csv('temp.csv', mode = 'overwrite', header = True)\n",
    "d2 = spark.read.csv('temp.csv', header = True, inferSchema = True)\n",
    "# d2 doesn't have color column\n",
    "d2.filter(\"Color = 'Black'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|SalesOrderID|\n",
      "+------------+\n",
      "|       71902|\n",
      "|       71832|\n",
      "|       71915|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all orders that include at least one black product, \n",
    "# return the product SalesOrderID, Name, UnitPrice, and OrderQty\n",
    "\n",
    "# SELECT DISTINCT SalesOrderID\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# JOIN SalesLT.Product ON SalesOrderDetail.ProductID = Product.ProductID\n",
    "# WHERE Color = 'Black'\n",
    "\n",
    "dfDetail.join(dfProduct.filter(\"Color='Black'\"), 'ProductID') \\\n",
    "        .select('SalesOrderID') \\\n",
    "        .distinct() \\\n",
    "        .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many colors in the products?\n",
    "\n",
    "# SELECT COUNT(DISTINCT Color)\n",
    "# FROM SalesLT.Product\n",
    "\n",
    "dfProduct.select('Color').distinct().count()\n",
    "\n",
    "# It's 1 more than standard SQL.  In standard SQL, COUNT() does not count NULLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71867|             858.9|\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "+------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order, \n",
    "# return SalesOrderID and total price (column name should be ‘totalprice’)\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail.OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|        TotalPrice|\n",
      "+------------+------------------+\n",
      "|       71902|59894.209199999976|\n",
      "|       71832|      28950.678108|\n",
      "|       71858|11528.844000000001|\n",
      "+------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|SalesOrderID|  TotalPrice|\n",
      "+------------+------------+\n",
      "|       71902|   26677.884|\n",
      "|       71832|16883.748108|\n",
      "|       71938|   33779.448|\n",
      "+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the total price on the black products of each order where the total price > 10000\n",
    "\n",
    "# SELECT SalesOrderID, SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) AS TotalPrice\n",
    "# FROM SalesLT.SalesOrderDetail, SalesLT.Product\n",
    "# WHERE SalesLT.SalesOrderDetail.ProductID = SalesLT.Product.ProductID AND Color = 'Black'\n",
    "# GROUP BY SalesOrderID\n",
    "# HAVING SUM(UnitPrice*OrderQty*(1-UnitPriceDiscount)) > 10000\n",
    "\n",
    "dfDetail.select('*', (dfDetail.UnitPrice * dfDetail. OrderQty\n",
    "                      * (1 - dfDetail.UnitPriceDiscount)).alias('netprice'))\\\n",
    "        .join(dfProduct.where(\"Color = 'Black'\"), 'ProductID') \\\n",
    "        .groupBy('SalesOrderID').sum('netprice') \\\n",
    "        .withColumnRenamed('sum(netprice)', 'TotalPrice')\\\n",
    "        .where('TotalPrice > 10000')\\\n",
    "        .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+-------------+\n",
      "|CustomerID|FirstName|    LastName|sum(OrderQty)|\n",
      "+----------+---------+------------+-------------+\n",
      "|     30050|  Krishna|Sunkammurali|           89|\n",
      "|     29796|      Jon|      Grande|           65|\n",
      "|     29957|    Kevin|         Liu|           62|\n",
      "+----------+---------+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each customer, find the total quantity of black products bought.\n",
    "# Report CustomerID, FirstName, LastName, and total quantity\n",
    "\n",
    "# select saleslt.customer.customerid, FirstName, LastName, sum(orderqty)\n",
    "# from saleslt.customer\n",
    "# left outer join \n",
    "# (\n",
    "# saleslt.salesorderheader\n",
    "# join saleslt.salesorderdetail\n",
    "# on saleslt.salesorderdetail.salesorderid = saleslt.salesorderheader.salesorderid\n",
    "# join saleslt.product\n",
    "# on saleslt.product.productid = saleslt.salesorderdetail.productid and color = 'black'\n",
    "# )\n",
    "# on saleslt.customer.customerid = saleslt.salesorderheader.customerid\n",
    "# group by saleslt.customer.customerid, FirstName, LastName\n",
    "# order by sum(orderqty) desc\n",
    "\n",
    "d1 = dfDetail.join(dfProduct, 'ProductID')\\\n",
    "             .where('Color = \"Black\"') \\\n",
    "             .join(dfHeader, 'SalesOrderID')\\\n",
    "             .groupBy('CustomerID').sum('OrderQty')\n",
    "dfCustomer.join(d1, 'CustomerID', 'left_outer')\\\n",
    "          .select('CustomerID', 'FirstName', 'LastName', 'sum(OrderQty)')\\\n",
    "          .orderBy('sum(OrderQty)', ascending=False)\\\n",
    "          .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "### Embed SQL queries\n",
    "\n",
    "You can also run SQL queries over dataframes once you register them as temporary tables within the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the dataframe as a temporary view called HVAC\n",
    "df.createOrReplaceTempView('HVAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "|         1|         M1|         25|     AC1000|         USA|\n",
      "|         2|         M2|         27|     FN39TG|      France|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|\n",
      "|         4|         M4|         17|     GG1919|     Finland|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|\n",
      "|        10|        M10|         23|    ACMAX22|       China|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|\n",
      "|        14|        M14|         17|     GG1919|     Germany|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|\n",
      "|        19|        M19|         14|     GG1919|      Canada|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|\n",
      "+----------+-----------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HVACproduct|count(1)|\n",
      "+-----------+--------+\n",
      "|    ACMAX22|       3|\n",
      "|     AC1000|       3|\n",
      "|     JDNS77|       4|\n",
      "|     FN39TG|       4|\n",
      "|     GG1919|       4|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can even mix DataFrame API with SQL:\n",
    "df.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\n",
    "spark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|HVACproduct|count|\n",
      "+-----------+-----+\n",
      "|    ACMAX22|    3|\n",
      "|     AC1000|    3|\n",
      "|     JDNS77|    4|\n",
      "|     FN39TG|    4|\n",
      "|     GG1919|    4|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\n",
    "d1.groupBy('HVACproduct').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   5|\n",
      "|         2|         M2|         27|     FN39TG|      France|   8|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   8|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   9|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|  11|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|  11|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  14|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|  11|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   8|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   7|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   9|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   9|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  14|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   9|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   8|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   8|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   7|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|  11|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   8|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|  11|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "slen = udf(lambda s: len(s)+2, IntegerType())\n",
    "df.select('*', slen(df['Country']).alias('slen')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|BuildingID|BuildingMgr|BuildingAge|HVACproduct|     Country|slen|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "|         1|         M1|         25|     AC1000|         USA|   3|\n",
      "|         2|         M2|         27|     FN39TG|      France|   6|\n",
      "|         3|         M3|         28|     JDNS77|      Brazil|   6|\n",
      "|         4|         M4|         17|     GG1919|     Finland|   7|\n",
      "|         5|         M5|          3|    ACMAX22|   Hong Kong|   9|\n",
      "|         6|         M6|          9|     AC1000|   Singapore|   9|\n",
      "|         7|         M7|         13|     FN39TG|South Africa|  12|\n",
      "|         8|         M8|         25|     JDNS77|   Australia|   9|\n",
      "|         9|         M9|         11|     GG1919|      Mexico|   6|\n",
      "|        10|        M10|         23|    ACMAX22|       China|   5|\n",
      "|        11|        M11|         14|     AC1000|     Belgium|   7|\n",
      "|        12|        M12|         26|     FN39TG|     Finland|   7|\n",
      "|        13|        M13|         25|     JDNS77|Saudi Arabia|  12|\n",
      "|        14|        M14|         17|     GG1919|     Germany|   7|\n",
      "|        15|        M15|         19|    ACMAX22|      Israel|   6|\n",
      "|        16|        M16|         23|     AC1000|      Turkey|   6|\n",
      "|        17|        M17|         11|     FN39TG|       Egypt|   5|\n",
      "|        18|        M18|         25|     JDNS77|   Indonesia|   9|\n",
      "|        19|        M19|         14|     GG1919|      Canada|   6|\n",
      "|        20|        M20|         19|    ACMAX22|   Argentina|   9|\n",
      "+----------+-----------+-----------+-----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('slen', lambda s: len(s), IntegerType())\n",
    "spark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Flexible Data Model\n",
    "\n",
    "Sample data file at\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/products.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dimensions: struct (nullable = true)\n",
      " |    |-- height: double (nullable = true)\n",
      " |    |-- length: double (nullable = true)\n",
      " |    |-- width: double (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- warehouseLocation: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../data/products.json')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "|    dimensions| id|            name|price|       tags|warehouseLocation|\n",
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "|[9.5,7.0,12.0]|  2|An ice sculpture| 12.5|[cold, ice]|    [-78.75,20.4]|\n",
      "| [1.0,3.1,1.0]|  3|    A blue mouse| 25.5|       null|     [54.4,-32.7]|\n",
      "+--------------+---+----------------+-----+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested fields\n",
    "\n",
    "df.select(df['dimensions.height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "|   1.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   9.5|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dimensions.height')\\\n",
    "  .filter(\"tags[0] = 'cold' AND warehouseLocation.latitude < 0\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dimensions=Row(height=9.5, length=7.0, width=12.0), id=2, name=u'An ice sculpture', price=12.5, tags=[u'cold', u'ice'], warehouseLocation=Row(latitude=-78.75, longitude=20.4)), Row(dimensions=Row(height=1.0, length=3.1, width=1.0), id=3, name=u'A blue mouse', price=25.5, tags=None, warehouseLocation=Row(latitude=54.4, longitude=-32.7))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Converting between RDD and DataFrame\n",
    "\n",
    "Sample data file at:\n",
    "\n",
    "https://www.cse.ust.hk/msbd5003/data/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Michael', 29), (u'Andy', 30), (u'Justin', 19)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"../data/people.txt\")\n",
    "\n",
    "def parse(l):\n",
    "    a = l.split(',')\n",
    "    return (a[0], int(a[1]))\n",
    "\n",
    "rdd = lines.map(parse)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|     _1| _2|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples, schema is inferred\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of tuples with column names, type is inferred\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the DataFrame from an RDD of Rows, type is given in the Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "rdd_rows = rdd.map(lambda p: Row(name = p[0], age = p[1]))\n",
    "df = spark.createDataFrame(rdd_rows)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age| name|\n",
      "+----+-----+\n",
      "|  11|Alice|\n",
      "|null|  Bob|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row fields with types incompatible with that of previous rows will be turned into nulls\n",
    "row1 = Row(name=\"Alice\", age=11)\n",
    "row2 = Row(name=\"Bob\", age='12')\n",
    "rdd_rows = sc.parallelize([row1, row2])\n",
    "df1 = spark.createDataFrame(rdd_rows)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Name: Justin']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd returns the content as an RDD of Rows\n",
    "teenagers = df.filter('age >= 13 and age <= 19')\n",
    "\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name)\n",
    "teenNames.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "DataFrames are stored using columnar storage with compression\n",
    "\n",
    "RDDs are stored using row storage without compression\n",
    "\n",
    "The RDD view of DataFrame just provides an interface, the Row objects are constructed on the fly and do not necessarily represent the internal storage format of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "|  5|  5|\n",
      "|  6|  6|\n",
      "|  7|  7|\n",
      "|  8|  8|\n",
      "|  9|  9|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = range(10)\n",
    "df = spark.createDataFrame(zip(data, data))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "|  4|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  0|  0|\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#??? The 'closure' behaviour in RDD doesn't seem to exist for DataFrames\n",
    "\n",
    "x = 5\n",
    "df1 = df.filter(df._1 < x)\n",
    "df1.show()\n",
    "x = 3\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter (isnotnull(_1#773L) && (_1#773L < 5))\n",
      "+- Scan ExistingRDD[_1#773L,_2#774L]\n"
     ]
    }
   ],
   "source": [
    "# Because of the Catalyst optimizer !\n",
    "\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [((_1#773L * 2) + 4) AS ((((_1 * 2) + 2) + 1) + 1)#794L]\n",
      "+- Scan ExistingRDD[_1#773L,_2#774L]\n"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return x/2\n",
    "x = 5\n",
    "df1 = df.select(df._1 * 2 + f() + 1 + 1)\n",
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "x = 5\n",
    "a = rdd.filter(lambda z: z < x)\n",
    "print a.take(10)\n",
    "x = 3\n",
    "print a.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "\n",
    "df.foreach(increment_counter)\n",
    "\n",
    "print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.13\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
